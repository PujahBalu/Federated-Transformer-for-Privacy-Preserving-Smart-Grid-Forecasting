# Federated Transformer for Privacy-Preserving Smart Grid Forecasting

## 🔍 Overview
This project demonstrates how to apply a **Transformer-based model** in a **federated learning** setting using the Flower framework, simulating a **privacy-preserving smart grid forecasting** environment.

## 🎯 Objectives
- Forecast smart meter energy consumption using TransformerXL.
- Simulate a federated learning environment using Flower.
- Implement basic differential privacy mechanisms.
- Compare performance: individual client models vs. central/global model.

## 📁 Structure
- `notebooks/`: Jupyter notebooks for testing and visualization
- `src/`: Core federated learning and model training code
- `images/`: Architecture diagrams and performance plots
- `data/`: Simulated or partitioned dataset
- `logs/`: Training logs

## 🧰 Tech Stack
- Python
- PyTorch / HuggingFace
- Flower (Federated Learning)
- TransformerXL
- Matplotlib, Pandas, Scikit-learn

## 🚀 Getting Started
```bash
pip install -r requirements.txt
```

## 📊 Output
- Federated training logs
- Accuracy dashboard across clients
- Differential privacy noise visualization

## 📄 License
MIT License
