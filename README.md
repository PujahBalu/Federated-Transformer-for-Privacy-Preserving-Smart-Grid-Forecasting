# Federated Transformer for Privacy-Preserving Smart Grid Forecasting

## ğŸ” Overview
This project demonstrates how to apply a **Transformer-based model** in a **federated learning** setting using the Flower framework, simulating a **privacy-preserving smart grid forecasting** environment.

## ğŸ¯ Objectives
- Forecast smart meter energy consumption using TransformerXL.
- Simulate a federated learning environment using Flower.
- Implement basic differential privacy mechanisms.
- Compare performance: individual client models vs. central/global model.

## ğŸ“ Structure
- `notebooks/`: Jupyter notebooks for testing and visualization
- `src/`: Core federated learning and model training code
- `images/`: Architecture diagrams and performance plots
- `data/`: Simulated or partitioned dataset
- `logs/`: Training logs

## ğŸ§° Tech Stack
- Python
- PyTorch / HuggingFace
- Flower (Federated Learning)
- TransformerXL
- Matplotlib, Pandas, Scikit-learn

## ğŸš€ Getting Started
```bash
pip install -r requirements.txt
```

## ğŸ“Š Output
- Federated training logs
- Accuracy dashboard across clients
- Differential privacy noise visualization

## ğŸ“„ License
MIT License
